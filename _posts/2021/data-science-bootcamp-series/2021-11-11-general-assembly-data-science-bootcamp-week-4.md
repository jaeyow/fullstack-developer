---
layout: posts
title: Data Science Bootcamp - Week 4
excerpt: Fourth week into the GA Data Science bootcamp, and we find out why we have to do data visualizations at all
modified: 2021-11-10
date: 2021-11-10
tags: [General Assembly Bootcamp, Data Science]
header: 
  overlay_image: /images/general-assembly/sajad-nori-xihagoynhn4-unsplash.jpg
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
comments: true
published: true
---
<section id="table-of-contents">
  <header>
    <h3>Overview</h3>
  </header>
  <div id="drawer" markdown="1">
  *  Auto generated table of contents
  {:toc}
  </div>
</section>

#### Part of the [General Assembly Data Science Bootcamp Series](../tags/#general-assembly-bootcamp)

## Why bother with data visualization?

While summary statistics (eg. sum, mean, stdev) are important concepts in the study of data science, they are not enough when you want to see a more complete understanding of your data. 

<figure>
	<a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet">
    <img src="../images/general-assembly/anscombs-quartet.png">
  </a>
	<figcaption>© Copyright General Assembly course notes</figcaption>
</figure>

Take for example the summary statistics data in the table above. All the summary statistics indicate that all the data sets are identical. Or are they? You'll see in the charts below, that could be further from the truth. They are all totally different, if not for the visualization with the charts, we will be none the wiser!

This is known as [Anscombe's Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet), which was first constructed by the statistician Anscombe in 1973. He wanted to demonstrate the need to graphing the data before analyzing it, and the effect of outliers on these statistical properties. 

<figure>
	<a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet">
    <img src="../images/general-assembly/anscombs-quartet-visualization.png">
  </a>
	<figcaption>© Copyright General Assembly course notes</figcaption>
</figure>

The above example highlights the shortcomings of summary statistics alone. It also shows the effects of outliers on these summary data. We are visual beings, and given the table above, it would not have given the impact and understanding that the charts would have easily conveyed.

## Week 4 - Python visualizations, aka throw away visualizations

In conjunction with exploratory data analysis and slicing and dicing your data, and in this case we use the most popular language and library for data scientists - [Python](https://www.python.org/) and [Pandas](https://pandas.pydata.org/), we will also run the data through a series of visualizations. We like to see patterns at a high level, and evaluate early on if we can continue, because part of data science is ensuring we have enough data and if the quality of the data is good enough. 

<figure>
	<a href="../images/general-assembly/seaborn-sample-pairplot.png">
    <img src="../images/general-assembly/seaborn-sample-pairplot.png">
  </a>
	<figcaption>© Copyright General Assembly course notes</figcaption>
</figure>

The charts above were generated by a popular python based library called [Seaborn](https://seaborn.pydata.org) which is based on yet another python visualization library called [matplotlib](https://matplotlib.org/). Both these libraries enable the data scientist to easily create many different types of visualizations straight from their Jupyter notebooks.

When working with your visualization, specially in your Jupyter notebooks, the preference is to create many, and we were encouraged to plot as many charts as we can, the idea being these will be treated as throw away charts. These visualizations were created for the sole purpose of finding pattern at the early stages of the data science end-to-end process.

## Capstone Project proposal due this coming Monday

I still have the whole weekend to complete my Capstone project proposal, as well as the Unit 2 assignment. This will be a busy weekend. I will still be updating the proposal draft below, but I have decided that my project will be using the Formula 1 Racing dataset. 

### Formula 1 Dataset

Ever since the first season of [Drive to Survive](https://en.wikipedia.org/wiki/Formula_1:_Drive_to_Survive), I've been captivated by the drama and excitement that is Formula 1. I've been consuming this public API in some of my past blog posts ([DynamoDB and Single-Table Design](https://fullstackdeveloper.tips/so-how-do-you-start-with-dynamodb/), [Simple GraphQL consumer with Apollo Client](https://fullstackdeveloper.tips/easy-graphql-consumer-with-apollo-client/)) and I thought it was fitting to continue this trend and explore the insights and predictions that can be gleaned from it:

- predict the winner of a race
- predict the pole sitter in the qualifying race
- predict who will be the fastest lap
- predict which team will be fastest pit stop
- explore the effect of the weather on the outcome of the rece
- who wins the constructor at the end of the year
- who is the last place in the next race

## Resources
- [Anscombe's Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet){:target="_blank"}
- [Anscombe's Quartet Jupyter Notebook](https://deepnote.com/project/pythontutorial-Kq6QLDjKTUqpGQj9nw4VHw/%2Fchapter4%2FAnscombes-quartet.ipynb){:target="_blank"}
- [Pandas - Open source data analysis and manipulation tool](https://pandas.pydata.org/){:target="_blank"}
- [Seaborn - Statistical Data Visualizations](https://seaborn.pydata.org/#)